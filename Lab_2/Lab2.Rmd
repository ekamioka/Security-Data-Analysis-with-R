---
title: "Security Data Analysis with R - Introduction to time series analysis"
author: "Eduardo Kamioka"
date: "September 1, 2015"
output: html_document
---
##Goal

In this lab, the techniques in the first lab are built on to do basic timeseries analysis (graphing and summarization). For the first part of the lab some Bro data (http.log) is used. Additionally the groupby() capability of data frames will be examined. 

In the second part of the lab (2.1) honeypot data will be used to demonstrate looking for patterns over time in honeypot data. More timeseries graphing will be covered as well as a technique for determining if two variables are correlated over time. Make sure to let us know if you find anything interesting in the data!

##Requirements
*R
Matplotlib
GeoIP

##Data

Full dataset available here: http://www.secrepo.com/Security-Data-Analysis/Lab_2/http.log.zip. This is the http.log referenced in this lab (Lab 2).
Honeypot data is available here: http://www.secrepo.com/honeypot/honeypot.json.zip . This is the honeypot.json referenced in this lab (Lab 2.1). You must unzip honeypot.json.zip prior to running through the lab.

#Lab 2 - Introduction

With this lab data grouping and graphing will be explored. There is always a great deal of information you can gather by grouping and comparing various columns within a dataframe. In addition for data summarization graphing is an important tool to have.

HTTP data will be used for this exercise. The data was generated from various PCAPs that have been collected that contain both legitimate traffic as well as traffic relating to exploit kits. While no malicious traffic is contained within the log file there are malicious domains and URLS (it's recommended you don't visit them). While this traffic was generated by running Bro over a series of PCAPS, similar data can be obtained from various Web Proxies, this is a nice cross over example of what is possible with your own data.

Some goals will be understand when the data was generated, what systems generated, high-level stats about the traffic, and the types of data transferred within the connections.

```{r, echo=FALSE}
setwd('~/workspace/Security-Data-Analysis-with-R/Lab_2')
```

##File Input

Using what was learned in the last lab creating the log (csv) file provided.
Hints:

*The file name is in the current directory and is called http.log
*There is no header to the file
*It's [TAB] seperated
*The fields are: 'ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_h', 'id.resp_p', 'trans_depth', 'method', 'host', 'uri', 'referrer', 'user_agent', 'request_body_len', 'response_body_len', 'status_code', 'status_msg', 'info_code', 'info_msg', 'filename', 'tags', 'username', 'password', 'proxied', 'orig_fuids', 'orig_mime_types', 'resp_fuids', 'resp_mime_types', 'sample'


```{r}
column_names = c('ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_h', 'id.resp_p', 'trans_depth', 'method', 'host', 'uri', 'referrer', 'user_agent', 'request_body_len', 'response_body_len', 'status_code', 'status_msg', 'info_code', 'info_msg', 'filename', 'tags', 'username', 'password', 'proxied', 'orig_fuids', 'orig_mime_types', 'resp_fuids', 'resp_mime_types', 'sample')

http_df = read.csv(file = '../data/http.log', sep = '\t', header = FALSE, col.names = column_names)
```

##Clean-up the timestamp

Now that you've got the data imported, cleanup up the timestamp column ts. Don't forget to re-assign back to the ts column.
```{r}

http_df$ts = as.POSIXlt.POSIXct(http_df$ts)

```

After the assignment a quick head() is performed.
```{r}
head(http_df)
```

##Selecting Based on Index Values

With the default indexing in dataframes you can select elements or slices of elements based on numbers. With a time indexed dataframe various parts of dates, or whole dates, can be used to select rows.

A year can be used eg: 2012, a year and month '2012-02' or a year, month and day '2012-02-20'.
```{r}
head(
  http_df[http_df$ts >= '2012-02-20' & http_df$ts <= '2012-02-23',]
  )
```

##Time Resampling

Time indexed information can be resampled and summarized.

Below is a resampling on day D that will count up the number of occurences per day. Try various date selections to get a feel for how the sampling works and how it can be used to summarize data.
```{r}
summ_http_df = http_df[http_df$ts >= '2012-02-20' & http_df$ts <= '2012-02-23',]

aggregate(x = summ_http_df, by = list(strftime(summ_http_df$ts, format = '%Y-%m-%d')), FUN = table)

http_df['2012-02-20':'2012-02-23'].resample("D", how='count').head()
```

